{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacka (midterm) thon \n",
    "\n",
    "## Detecting Malicious URLs \n",
    "\n",
    "Today you are invited to repeat the path of researchers Detecting Malicious URLs.\n",
    "An anonymized 120-day subset of our ICML-09 data set.\n",
    "The data set consists of about 2.4 million URLs (examples) and 3.2 million features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download data using link below\n",
    "[Download Dataset](http://www.sysnet.ucsd.edu/projects/url/url_svmlight.tar.gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Description of Data (SVM-light)\n",
    "Uncompressing the archive url_svmlight.tar.gz will yield a directory url_svmlight/ containing the following files:\n",
    "\n",
    "1. **FeatureTypes**. A text file list of feature indices that correspond to real-valued features.\n",
    "2. **DayX.svm** (where X is an integer from 0 to 120) --- The data for day X in SVM-light format. A label of +1 corresponds to a malicious URL and -1 corresponds to a benign URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Read article\n",
    "Please familiarize yourself with original research article. It will give you required context.\n",
    "\n",
    "*\"**Beyond Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs**\"* \n",
    "\n",
    "*Justin Ma, Lawrence K. Saul, Stefan Savage, Geoffrey M. Voelker* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 121 files\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "files = glob.glob('./url_svmlight/*.svm')\n",
    "print(\"There are %d files\" % len(files))\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting url_svmlight,f size 0\n",
      "extracting url_svmlight/Day33.svm,f size 18674876\n",
      "extracting url_svmlight/Day32.svm,f size 18599211\n",
      "extracting url_svmlight/Day53.svm,f size 18963938\n",
      "extracting url_svmlight/Day20.svm,f size 18633460\n",
      "extracting url_svmlight/Day7.svm,f size 18777054\n",
      "extracting url_svmlight/Day117.svm,f size 18106370\n",
      "max X = 3231952, max y dimension = 20000\n"
     ]
    }
   ],
   "source": [
    "uri = ('./url_svmlight.tar.gz')\n",
    "tar = tarfile.open(uri, \"r:gz\")\n",
    "max_obs = 0\n",
    "max_vars = 0\n",
    "i = 0\n",
    "split = 5\n",
    "for tarinfo in tar:\n",
    "    print(\"extracting %s,f size %s\" % (tarinfo.name, tarinfo.size))\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f)\n",
    "        max_vars = np.maximum(max_vars, X.shape[0])\n",
    "        max_obs = np.maximum(max_obs, X.shape[1])\n",
    "    if i > split:\n",
    "        break\n",
    "    i+=1\n",
    "print(\"max X = %s, max y dimension = %s\" % (max_obs, max_vars)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.97      0.99      0.98     13979\n",
      "          1       0.98      0.93      0.95      6021\n",
      "\n",
      "avg / total       0.97      0.97      0.97     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "classes = [-1,1] # 1_:url- safety, -1: url- non-safety\n",
    "sgd = SGDClassifier(loss='log')\n",
    "n_features = 3231952\n",
    "split = 5\n",
    "i = 0\n",
    "for tarinfo in tar:\n",
    "    if i > split:\n",
    "        break\n",
    "    if tarinfo.isfile():\n",
    "        f = tar.extractfile(tarinfo.name)\n",
    "        X,y = load_svmlight_file(f,n_features=n_features)\n",
    "        if i < split:\n",
    "            sgd.partial_fit(X,y, classes = classes)\n",
    "        if i == split:\n",
    "            print (classification_report(sgd.predict(X),y))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading criteria\n",
    "- Complete solution - 50%\n",
    "- F1 Score - 40%\n",
    "    - The first champion champion get 40%\n",
    "    - Worst champion get 20%\n",
    "    - All others are on a scale between them\n",
    "- Code Style - 10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadline\n",
    "9:00 AM MSK Monday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train, test\n",
    "- Upload data (you can use template above)\n",
    "- Separate your dataset into train and test subsets of observations\n",
    "- Use the 8:2 ratio: 80% train set, 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "def train_test_split(train_size = 0.8):\n",
    "    \n",
    "    train = open('./url_svmlight/train.svm','w')\n",
    "    test  = open('./url_svmlight/test.svm','w')\n",
    "    summ = 0\n",
    "    for i in range(121):\n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        print(\"file: \" + str(i))\n",
    "        \n",
    "        q = [0, 0]\n",
    "        \n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "            if(a[0] == \"-1\"):\n",
    "                q[0] += 1\n",
    "            else:\n",
    "                q[1] += 1\n",
    "        \n",
    "        inn.close()\n",
    "        summ += q[0] + q[1]\n",
    "        check = [int(train_size * q[0]),int(train_size * q[1])]\n",
    "        start = [0, 0]\n",
    "        \n",
    "        inn = open('./url_svmlight/Day' + str(i) + '.svm','r')\n",
    "        \n",
    "        if (train_size * q[0]) % 1 >=0.5:\n",
    "            check[0] += 1\n",
    "        if (train_size * q[1]) % 1 >=0.5:\n",
    "            check[1] += 1\n",
    "        for line in inn:\n",
    "            a = line.split()\n",
    "\n",
    "            rand = random()\n",
    "            if rand > 0.5:\n",
    "                if a[0] == \"-1\":\n",
    "                    if start[0] + 1 <= check[0]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                else:\n",
    "                    if start[1] + 1 <= check[1]:\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "                    else:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "            else:\n",
    "                if(a[0] == \"-1\"):\n",
    "                    if q[0] > check[0]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[0] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[0] += 1\n",
    "                else:\n",
    "                    if q[1] > check[1]:\n",
    "                        test.write(line)\n",
    "                        test.write(\"\\n\")\n",
    "                        q[1] -= 1\n",
    "                    else :\n",
    "                        train.write(line)\n",
    "                        train.write(\"\\n\")\n",
    "                        start[1] += 1\n",
    "        print(\"finish: \" + str(i))\n",
    "    print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0\n",
      "finish: 0\n",
      "file: 1\n",
      "finish: 1\n",
      "file: 2\n",
      "finish: 2\n",
      "file: 3\n",
      "finish: 3\n",
      "file: 4\n",
      "finish: 4\n",
      "file: 5\n",
      "finish: 5\n",
      "file: 6\n",
      "finish: 6\n",
      "file: 7\n",
      "finish: 7\n",
      "file: 8\n",
      "finish: 8\n",
      "file: 9\n",
      "finish: 9\n",
      "file: 10\n",
      "finish: 10\n",
      "file: 11\n",
      "finish: 11\n",
      "file: 12\n",
      "finish: 12\n",
      "file: 13\n",
      "finish: 13\n",
      "file: 14\n",
      "finish: 14\n",
      "file: 15\n",
      "finish: 15\n",
      "file: 16\n",
      "finish: 16\n",
      "file: 17\n",
      "finish: 17\n",
      "file: 18\n",
      "finish: 18\n",
      "file: 19\n",
      "finish: 19\n",
      "file: 20\n",
      "finish: 20\n",
      "file: 21\n",
      "finish: 21\n",
      "file: 22\n",
      "finish: 22\n",
      "file: 23\n",
      "finish: 23\n",
      "file: 24\n",
      "finish: 24\n",
      "file: 25\n",
      "finish: 25\n",
      "file: 26\n",
      "finish: 26\n",
      "file: 27\n",
      "finish: 27\n",
      "file: 28\n",
      "finish: 28\n",
      "file: 29\n",
      "finish: 29\n",
      "file: 30\n",
      "finish: 30\n",
      "file: 31\n",
      "finish: 31\n",
      "file: 32\n",
      "finish: 32\n",
      "file: 33\n",
      "finish: 33\n",
      "file: 34\n",
      "finish: 34\n",
      "file: 35\n",
      "finish: 35\n",
      "file: 36\n",
      "finish: 36\n",
      "file: 37\n",
      "finish: 37\n",
      "file: 38\n",
      "finish: 38\n",
      "file: 39\n",
      "finish: 39\n",
      "file: 40\n",
      "finish: 40\n",
      "file: 41\n",
      "finish: 41\n",
      "file: 42\n",
      "finish: 42\n",
      "file: 43\n",
      "finish: 43\n",
      "file: 44\n",
      "finish: 44\n",
      "file: 45\n",
      "finish: 45\n",
      "file: 46\n",
      "finish: 46\n",
      "file: 47\n",
      "finish: 47\n",
      "file: 48\n",
      "finish: 48\n",
      "file: 49\n",
      "finish: 49\n",
      "file: 50\n",
      "finish: 50\n",
      "file: 51\n",
      "finish: 51\n",
      "file: 52\n",
      "finish: 52\n",
      "file: 53\n",
      "finish: 53\n",
      "file: 54\n",
      "finish: 54\n",
      "file: 55\n",
      "finish: 55\n",
      "file: 56\n",
      "finish: 56\n",
      "file: 57\n",
      "finish: 57\n",
      "file: 58\n",
      "finish: 58\n",
      "file: 59\n",
      "finish: 59\n",
      "file: 60\n",
      "finish: 60\n",
      "file: 61\n",
      "finish: 61\n",
      "file: 62\n",
      "finish: 62\n",
      "file: 63\n",
      "finish: 63\n",
      "file: 64\n",
      "finish: 64\n",
      "file: 65\n",
      "finish: 65\n",
      "file: 66\n",
      "finish: 66\n",
      "file: 67\n",
      "finish: 67\n",
      "file: 68\n",
      "finish: 68\n",
      "file: 69\n",
      "finish: 69\n",
      "file: 70\n",
      "finish: 70\n",
      "file: 71\n",
      "finish: 71\n",
      "file: 72\n",
      "finish: 72\n",
      "file: 73\n",
      "finish: 73\n",
      "file: 74\n",
      "finish: 74\n",
      "file: 75\n",
      "finish: 75\n",
      "file: 76\n",
      "finish: 76\n",
      "file: 77\n",
      "finish: 77\n",
      "file: 78\n",
      "finish: 78\n",
      "file: 79\n",
      "finish: 79\n",
      "file: 80\n",
      "finish: 80\n",
      "file: 81\n",
      "finish: 81\n",
      "file: 82\n",
      "finish: 82\n",
      "file: 83\n",
      "finish: 83\n",
      "file: 84\n",
      "finish: 84\n",
      "file: 85\n",
      "finish: 85\n",
      "file: 86\n",
      "finish: 86\n",
      "file: 87\n",
      "finish: 87\n",
      "file: 88\n",
      "finish: 88\n",
      "file: 89\n",
      "finish: 89\n",
      "file: 90\n",
      "finish: 90\n",
      "file: 91\n",
      "finish: 91\n",
      "file: 92\n",
      "finish: 92\n",
      "file: 93\n",
      "finish: 93\n",
      "file: 94\n",
      "finish: 94\n",
      "file: 95\n",
      "finish: 95\n",
      "file: 96\n",
      "finish: 96\n",
      "file: 97\n",
      "finish: 97\n",
      "file: 98\n",
      "finish: 98\n",
      "file: 99\n",
      "finish: 99\n",
      "file: 100\n",
      "finish: 100\n",
      "file: 101\n",
      "finish: 101\n",
      "file: 102\n",
      "finish: 102\n",
      "file: 103\n",
      "finish: 103\n",
      "file: 104\n",
      "finish: 104\n",
      "file: 105\n",
      "finish: 105\n",
      "file: 106\n",
      "finish: 106\n",
      "file: 107\n",
      "finish: 107\n",
      "file: 108\n",
      "finish: 108\n",
      "file: 109\n",
      "finish: 109\n",
      "file: 110\n",
      "finish: 110\n",
      "file: 111\n",
      "finish: 111\n",
      "file: 112\n",
      "finish: 112\n",
      "file: 113\n",
      "finish: 113\n",
      "file: 114\n",
      "finish: 114\n",
      "file: 115\n",
      "finish: 115\n",
      "file: 116\n",
      "finish: 116\n",
      "file: 117\n",
      "finish: 117\n",
      "file: 118\n",
      "finish: 118\n",
      "file: 119\n",
      "finish: 119\n",
      "file: 120\n",
      "finish: 120\n",
      "2396130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "train_test_split()\n",
    "\n",
    "data = None\n",
    "n_features = 3231961\n",
    "\n",
    "\n",
    "data, target = load_svmlight_file(\"./url_svmlight/train.svm\",n_features=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Find out whether it is possible to reduce the dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faritgaleev/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "              fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
       "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier()\n",
    "pac.fit(data,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get the quality\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       -1.0   0.993186  0.990672  0.991927    321609\n",
      "        1.0   0.981064  0.986131  0.983591    157617\n",
      "\n",
      "avg / total   0.989199  0.989178  0.989185    479226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data, target = load_svmlight_file(\"./url_svmlight/test.svm\",n_features=n_features)\n",
    "print (classification_report(pac.predict(data),target, digits = 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
